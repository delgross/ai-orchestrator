from fastapi import FastAPI
from pydantic import BaseModel
import httpx
import json

app = FastAPI()

# Set your fallback model here
DEFAULT_FALLBACK_MODEL = "mistral"

############################################################
# Data Models
############################################################

class Message(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    model: str
    messages: list[Message]

############################################################
# Ollama Helpers
############################################################

async def ollama_running() -> bool:
    """Return True if Ollama is reachable."""
    try:
        async with httpx.AsyncClient() as client:
            r = await client.get("http://127.0.0.1:11434/api/tags", timeout=1.0)
            return r.status_code == 200
    except Exception:
        return False


async def model_exists(model_name: str) -> bool:
    """Return True if the model exists locally in Ollama."""
    try:
        async with httpx.AsyncClient() as client:
            r = await client.get("http://127.0.0.1:11434/api/tags")
            if r.status_code != 200:
                return False
            data = r.json()
            available = [m["name"] for m in data.get("models", [])]
            return model_name in available
    except Exception:
        return False


async def call_ollama(model: str, messages):
    """
    Send a chat request to Ollama and force NON-streaming output
    so we always get a single valid JSON object.
    """
    payload = {
        "model": model,
        "messages": messages,
        "stream": False    # <- FORCE NON-STREAMING MODE
    }

    async with httpx.AsyncClient() as client:
        r = await client.post(
            "http://127.0.0.1:11434/api/chat",
            json=payload,
            timeout=120.0
        )

        # If this fails, Ollama itself returned bad JSON
        try:
            return r.json()
        except Exception:
            return {
                "error": "Ollama returned non-JSON output (unexpected in non-stream mode)",
                "raw": r.text
            }

############################################################
# Chat Completion Endpoint
############################################################

@app.post("/v1/chat/completions")
async def chat(req: ChatRequest):
    """Main OpenAI-compatible chat endpoint."""

    # Convert Pydantic objects → dict list
    messages = [{"role": m.role, "content": m.content} for m in req.messages]

    # 1. Ensure Ollama is running
    if not await ollama_running():
        return {
            "error": "Ollama server not reachable.",
            "suggestion": "Start Ollama Desktop or run: ollama serve"
        }

    # 2. Requested model exists
    if await model_exists(req.model):
        return await call_ollama(req.model, messages)

    # 3. Model missing → fallback
    if DEFAULT_FALLBACK_MODEL and await model_exists(DEFAULT_FALLBACK_MODEL):
        return {
            "warning": f"Model '{req.model}' not found. Using fallback '{DEFAULT_FALLBACK_MODEL}'.",
            "response": await call_ollama(DEFAULT_FALLBACK_MODEL, messages)
        }

    # 4. No models available → clean message
    return {
        "error": (
            f"Model '{req.model}' not found, and fallback "
            f"'{DEFAULT_FALLBACK_MODEL}' is not available."
        ),
        "suggestion": "Run `ollama list` to view installed models."
    }
