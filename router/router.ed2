# ~/ai/router/router.py
from __future__ import annotations

import json
import os
import time
import uuid
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple

import logging

import httpx
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel

# -----------------------------------------------------------------------------
# Config
# -----------------------------------------------------------------------------
OLLAMA_BASE = os.getenv("OLLAMA_BASE", "http://127.0.0.1:11434").rstrip("/")
DEFAULT_FALLBACK_MODEL = os.getenv("DEFAULT_FALLBACK_MODEL", "mistral:latest")

PROVIDERS_YAML = os.getenv("PROVIDERS_YAML", "/Users/bee/ai/providers.yaml")

TAGS_TIMEOUT_S = float(os.getenv("OLLAMA_TAGS_TIMEOUT", "3.0"))
CHAT_TIMEOUT_S = float(os.getenv("OLLAMA_CHAT_TIMEOUT", "180.0"))
REMOTE_TIMEOUT_S = float(os.getenv("REMOTE_TIMEOUT", "180.0"))
REMOTE_MODELS_TIMEOUT_S = float(os.getenv("REMOTE_MODELS_TIMEOUT", "10.0"))

MODELS_CACHE_TTL_S = float(os.getenv("MODELS_CACHE_TTL_S", "600.0"))

CB_FAIL_THRESHOLD = int(os.getenv("CB_FAIL_THRESHOLD", "3"))
CB_OPEN_SECS = float(os.getenv("CB_OPEN_SECS", "30.0"))

# Agent-related config
AGENT_WEB_PROVIDER = os.getenv("AGENT_WEB_PROVIDER", "perplexity")
AGENT_WEB_MODEL = os.getenv("AGENT_WEB_MODEL", "sonar")

# External agent-runner entrypoint (your agent_runner FastAPI)
# Note: we actually POST to this URL directly (no extra /v1/chat/completions).
AGENT_RUNNER_URL = os.getenv(
    "AGENT_RUNNER_URL",
    "http://127.0.0.1:5460/v1/chat/completions",
).rstrip("/")

AGENT_MODEL_IDS = [
    "agent:web",
    "agent:mcp",
]

app = FastAPI(title="AI Gateway", version="0.4.2")

# -----------------------------------------------------------------------------
# Logging
# -----------------------------------------------------------------------------
logger = logging.getLogger("gateway")
if not logger.handlers:
    h = logging.StreamHandler()
    h.setFormatter(logging.Formatter("%(asctime)s %(levelname)s gateway %(message)s"))
    logger.addHandler(h)
logger.setLevel(logging.INFO)
logger.propagate = False


@app.on_event("startup")
async def _on_startup() -> None:
    logger.info(
        "startup",
        extra={
            "ollama_base": OLLAMA_BASE,
            "providers_yaml": PROVIDERS_YAML,
        },
    )


# -----------------------------------------------------------------------------
# OpenAI-ish request models
# -----------------------------------------------------------------------------
class Message(BaseModel):
    role: str
    content: str


class ChatRequest(BaseModel):
    model: str
    messages: List[Message]
    stream: bool = False

    # optional OpenAI-ish fields (passed through to OpenAI-compat providers)
    temperature: Optional[float] = None
    top_p: Optional[float] = None
    max_tokens: Optional[int] = None

    # extra fields some clients may send (ignored unless provider supports them)
    stop: Optional[Any] = None
    presence_penalty: Optional[float] = None
    frequency_penalty: Optional[float] = None

    # OpenAI tool calling (for remote providers + agent_runner)
    tools: Optional[List[Dict[str, Any]]] = None
    tool_choice: Optional[Any] = None


# -----------------------------------------------------------------------------
# Providers
# -----------------------------------------------------------------------------
@dataclass
class Provider:
    name: str
    type: str
    base_url: str
    api_key_env: Optional[str] = None
    default_headers: Dict[str, str] = field(default_factory=dict)
    models: List[str] = field(default_factory=list)

    def api_key(self) -> Optional[str]:
        if not self.api_key_env:
            return None
        return os.getenv(self.api_key_env)


def _load_providers() -> Dict[str, Provider]:
    providers: Dict[str, Provider] = {}

    # Always include local ollama (even if not in YAML)
    providers["ollama"] = Provider(
        name="ollama",
        type="ollama",
        base_url=OLLAMA_BASE,
        api_key_env=None,
    )

    if not PROVIDERS_YAML or not os.path.exists(PROVIDERS_YAML):
        return providers

    try:
        import yaml  # type: ignore
    except Exception as e:
        raise RuntimeError(
            f"PROVIDERS_YAML is set to {PROVIDERS_YAML}, but PyYAML is not installed. "
            f"Install it in your venv: pip install pyyaml. Underlying error: {e}"
        )

    with open(PROVIDERS_YAML, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f) or {}

    root = data.get("providers", {}) if isinstance(data, dict) else {}
    if not isinstance(root, dict):
        return providers

    for name, cfg in root.items():
        if not isinstance(name, str) or not isinstance(cfg, dict):
            continue

        name = name.strip()
        if not name:
            continue

        ptype = str(cfg.get("type", "")).strip()
        base_url = str(cfg.get("base_url", "")).strip().rstrip("/")
        if not ptype or not base_url:
            continue

        api_key_env = cfg.get("api_key_env")
        if api_key_env is not None:
            api_key_env = str(api_key_env).strip()

        default_headers = cfg.get("default_headers") or cfg.get("headers") or {}
        if not isinstance(default_headers, dict):
            default_headers = {}

        models_cfg = cfg.get("models") or {}
        models: List[str] = []
        if isinstance(models_cfg, dict):
            if isinstance(models_cfg.get("list"), list):
                models = [str(m) for m in models_cfg["list"] if isinstance(m, (str, int, float))]
        elif isinstance(models_cfg, list):
            models = [str(m) for m in models_cfg if isinstance(m, (str, int, float))]

        providers[name] = Provider(
            name=name,
            type=ptype,
            base_url=base_url,
            api_key_env=api_key_env,
            default_headers={str(k): str(v) for k, v in default_headers.items()},
            models=models,
        )

    return providers


PROVIDERS: Dict[str, Provider] = _load_providers()

# provider -> (expires_at, [models...])
REMOTE_MODELS_CACHE: Dict[str, Tuple[float, List[str]]] = {}


def _clear_models_cache() -> None:
    REMOTE_MODELS_CACHE.clear()


def _reload_providers_inplace() -> Dict[str, Provider]:
    global PROVIDERS
    PROVIDERS = _load_providers()
    _clear_models_cache()
    return PROVIDERS


# -----------------------------------------------------------------------------
# Ollama helpers
# -----------------------------------------------------------------------------
async def _get_tags() -> Dict[str, Any]:
    url = f"{OLLAMA_BASE}/api/tags"
    async with httpx.AsyncClient(timeout=TAGS_TIMEOUT_S) as client:
        r = await client.get(url)
        r.raise_for_status()
        return r.json()


async def ollama_running() -> bool:
    try:
        await _get_tags()
        return True
    except Exception:
        return False


async def list_ollama_models() -> List[str]:
    payload = await _get_tags()
    models = payload.get("models", [])
    if not isinstance(models, list):
        return []
    names: List[str] = []
    for m in models:
        if isinstance(m, dict) and isinstance(m.get("name"), str):
            names.append(m["name"])
    return names


def _normalize_ollama_model(requested: str, available: List[str]) -> Tuple[bool, str]:
    if requested in available:
        return True, requested
    if ":" not in requested:
        cand = f"{requested}:latest"
        if cand in available:
            return True, cand
    return False, requested


async def choose_ollama_model(requested: str) -> str:
    try:
        available = await list_ollama_models()
    except Exception as e:
        raise HTTPException(status_code=502, detail=f"Failed to reach Ollama at {OLLAMA_BASE}: {e}")

    ok, actual = _normalize_ollama_model(requested, available)
    if ok:
        return actual

    if DEFAULT_FALLBACK_MODEL:
        ok2, actual2 = _normalize_ollama_model(DEFAULT_FALLBACK_MODEL, available)
        if ok2:
            return actual2

    raise HTTPException(
        status_code=400,
        detail={
            "error": f"Model '{requested}' not found, and fallback '{DEFAULT_FALLBACK_MODEL}' is not available.",
            "suggestion": "Run `ollama list` to view installed models.",
        },
    )


async def ollama_chat_nonstream(model: str, messages: List[Dict[str, str]]) -> Dict[str, Any]:
    url = f"{OLLAMA_BASE}/api/chat"
    payload = {"model": model, "messages": messages, "stream": False}
    async with httpx.AsyncClient(timeout=CHAT_TIMEOUT_S) as client:
        r = await client.post(url, json=payload)
        r.raise_for_status()
        return r.json()


async def ollama_chat_stream_ndjson(model: str, messages: List[Dict[str, str]]):
    url = f"{OLLAMA_BASE}/api/chat"
    payload = {"model": model, "messages": messages, "stream": True}
    async with httpx.AsyncClient(timeout=None) as client:
        async with client.stream("POST", url, json=payload, timeout=CHAT_TIMEOUT_S) as resp:
            resp.raise_for_status()
            async for line in resp.aiter_lines():
                if not line:
                    continue
                try:
                    yield json.loads(line)
                except json.JSONDecodeError:
                    continue


# -----------------------------------------------------------------------------
# OpenAI-ish helpers (for Ollama conversion)
# -----------------------------------------------------------------------------
def _openai_completion_id() -> str:
    return f"chatcmpl-{uuid.uuid4().hex}"


def _openai_nonstream_response(model: str, content: str, created: int) -> Dict[str, Any]:
    return {
        "id": _openai_completion_id(),
        "object": "chat.completion",
        "created": created,
        "model": model,
        "choices": [
            {
                "index": 0,
                "message": {"role": "assistant", "content": content},
                "finish_reason": "stop",
            }
        ],
    }


def _sse(data_obj: Any) -> str:
    return f"data: {json.dumps(data_obj, ensure_ascii=False)}\n\n"


# -----------------------------------------------------------------------------
# OpenAI-compat remote provider helpers
# -----------------------------------------------------------------------------
def _split_model(model: str) -> Tuple[str, str]:
    """
    Returns (provider_name, model_name).

    Rules:
      - "openrouter:meta-llama/llama-3.1-8b-instruct" -> ("openrouter", "meta-llama/llama-3.1-8b-instruct")
      - "llama3:8b" -> ("ollama", "llama3:8b")   (default)
      - "agent:web" / "agent:mcp" -> handled before calling this
    """
    if ":" in model:
        pfx, rest = model.split(":", 1)
        if pfx in PROVIDERS and pfx != "ollama":
            return pfx, rest
    return "ollama", model


def _auth_headers(p: Provider) -> Dict[str, str]:
    headers = {"Content-Type": "application/json"}
    headers.update(p.default_headers or {})
    key = p.api_key()
    if key:
        headers["Authorization"] = f"Bearer {key}"
    return headers


async def _remote_list_models_openai_compat(p: Provider) -> List[str]:
    now = time.time()
    cached = REMOTE_MODELS_CACHE.get(p.name)
    if cached:
        expires_at, items = cached
        if now < expires_at:
            return list(items)

    fallback = list(p.models or [])

    if p.api_key_env and not p.api_key():
        REMOTE_MODELS_CACHE[p.name] = (now + MODELS_CACHE_TTL_S, fallback)
        return fallback

    url = f"{p.base_url}/models"
    try:
        async with httpx.AsyncClient(timeout=REMOTE_MODELS_TIMEOUT_S) as client:
            r = await client.get(url, headers=_auth_headers(p))
            if r.status_code >= 400:
                REMOTE_MODELS_CACHE[p.name] = (now + MODELS_CACHE_TTL_S, fallback)
                return fallback
            data = r.json()
            out: List[str] = []
            items = data.get("data", [])
            if isinstance(items, list):
                for it in items:
                    if isinstance(it, dict) and isinstance(it.get("id"), str):
                        out.append(it["id"])
            if not out:
                out = fallback
            REMOTE_MODELS_CACHE[p.name] = (now + MODELS_CACHE_TTL_S, out)
            return out
    except Exception:
        REMOTE_MODELS_CACHE[p.name] = (now + MODELS_CACHE_TTL_S, fallback)
        return fallback


async def _remote_chat_openai_compat_nonstream(p: Provider, model: str, req: ChatRequest) -> Dict[str, Any]:
    url = f"{p.base_url}/chat/completions"
    payload: Dict[str, Any] = {
        "model": model,
        "messages": [{"role": m.role, "content": m.content} for m in req.messages],
        "stream": False,
    }
    for k in ("temperature", "top_p", "max_tokens", "stop", "presence_penalty", "frequency_penalty"):
        v = getattr(req, k, None)
        if v is not None:
            payload[k] = v

    # forward tools/tool_choice for providers that support OpenAI tools
    if getattr(req, "tools", None) is not None:
        payload["tools"] = req.tools
    if getattr(req, "tool_choice", None) is not None:
        payload["tool_choice"] = req.tool_choice

    async with httpx.AsyncClient(timeout=REMOTE_TIMEOUT_S) as client:
        r = await client.post(url, headers=_auth_headers(p), json=payload)
        if r.status_code >= 400:
            try:
                raise HTTPException(status_code=r.status_code, detail=r.json())
            except HTTPException:
                raise
            except Exception:
                raise HTTPException(status_code=r.status_code, detail=r.text)
        return r.json()


async def _remote_chat_openai_compat_stream_passthrough(p: Provider, model: str, req: ChatRequest):
    url = f"{p.base_url}/chat/completions"
    payload: Dict[str, Any] = {
        "model": model,
        "messages": [{"role": m.role, "content": m.content} for m in req.messages],
        "stream": True,
    }
    for k in ("temperature", "top_p", "max_tokens", "stop", "presence_penalty", "frequency_penalty"):
        v = getattr(req, k, None)
        if v is not None:
            payload[k] = v

    # forward tools/tool_choice here as well (for streaming tool-calls)
    if getattr(req, "tools", None) is not None:
        payload["tools"] = req.tools
    if getattr(req, "tool_choice", None) is not None:
        payload["tool_choice"] = req.tool_choice

    async with httpx.AsyncClient(timeout=None) as client:
        async with client.stream("POST", url, headers=_auth_headers(p), json=payload, timeout=REMOTE_TIMEOUT_S) as resp:
            if resp.status_code >= 400:
                try:
                    body = await resp.aread()
                    raise HTTPException(status_code=resp.status_code, detail=body.decode("utf-8", "ignore"))
                except HTTPException:
                    raise
                except Exception:
                    raise HTTPException(status_code=resp.status_code, detail="Remote provider error")

            async for chunk in resp.aiter_bytes():
                if chunk:
                    yield chunk


# -----------------------------------------------------------------------------
# agent:web wrapper (non-stream for now)
# -----------------------------------------------------------------------------
async def _run_agent_web(req: ChatRequest) -> Dict[str, Any]:
    """
    Thin wrapper around a remote OpenAI-compatible provider (e.g. Perplexity)
    that already does web browsing/search.
    """
    provider_name = AGENT_WEB_PROVIDER
    model_name = AGENT_WEB_MODEL

    p = PROVIDERS.get(provider_name)
    if not p:
        raise HTTPException(
            status_code=500,
            detail=f"agent:web: provider '{provider_name}' not found in PROVIDERS. "
                   f"Set AGENT_WEB_PROVIDER env or fix providers.yaml."
        )

    if p.type != "openai_compat":
        raise HTTPException(
            status_code=500,
            detail=f"agent:web: provider '{provider_name}' is not openai_compat."
        )

    if p.api_key_env and not p.api_key():
        raise HTTPException(
            status_code=500,
            detail={
                "error": f"agent:web: missing API key for provider '{provider_name}'.",
                "suggestion": f"Set {p.api_key_env} in providers.env and restart the gateway.",
            },
        )

    if req.stream:
        raise HTTPException(
            status_code=400,
            detail="agent:web does not support stream yet (set stream=false).",
        )

    logger.info("agent:web dispatch", extra={"provider": provider_name, "model": model_name})
    return await _remote_chat_openai_compat_nonstream(p, model_name, req)


# -----------------------------------------------------------------------------
# Agent runner (MCP / orchestration entrypoint)
# -----------------------------------------------------------------------------
async def _call_agent_runner(req: ChatRequest) -> Dict[str, Any]:
    """
    Delegate the whole request to the external agent-runner service.

    The agent-runner is expected to expose a POST /v1/chat/completions endpoint
    that accepts an OpenAI-style chat payload (including tools) and returns
    an OpenAI-style response.
    """
    # Build a standard OpenAI-ish payload for the agent-runner
    payload: Dict[str, Any] = {
        "model": req.model,
        "messages": [{"role": m.role, "content": m.content} for m in req.messages],
        "stream": False,
    }

    for k in ("temperature", "top_p", "max_tokens", "stop", "presence_penalty", "frequency_penalty"):
        v = getattr(req, k, None)
        if v is not None:
            payload[k] = v

    # If a client sends tools/tool_choice directly to agent:mcp,
    # pass them through to agent-runner.
    if getattr(req, "tools", None) is not None:
        payload["tools"] = req.tools
    if getattr(req, "tool_choice", None) is not None:
        payload["tool_choice"] = req.tool_choice

    logger.info("agent:mcp dispatch", extra={"agent_runner_url": AGENT_RUNNER_URL})

    try:
        async with httpx.AsyncClient(timeout=REMOTE_TIMEOUT_S) as client:
            resp = await client.post(AGENT_RUNNER_URL, json=payload)
    except httpx.TimeoutException:
        logger.error("agent-runner timeout", extra={"url": AGENT_RUNNER_URL})
        raise HTTPException(status_code=504, detail="Timed out calling agent-runner.")
    except httpx.HTTPError as e:
        logger.error("agent-runner HTTP error", extra={"url": AGENT_RUNNER_URL, "error": str(e)})
        raise HTTPException(status_code=502, detail=f"Error calling agent-runner: {e}")

    if resp.status_code >= 400:
        try:
            detail = resp.json()
        except Exception:
            detail = resp.text
        logger.error("agent-runner non-200", extra={"status": resp.status_code, "detail": detail})
        raise HTTPException(status_code=resp.status_code, detail=detail)

    try:
        data = resp.json()
    except Exception as e:
        logger.error("agent-runner invalid JSON", extra={"error": str(e)})
        raise HTTPException(status_code=502, detail=f"Invalid JSON from agent-runner: {e}")

    return data


async def _handle_agent(req: ChatRequest) -> Dict[str, Any]:
    """
    Dispatch agent:* models.

    - agent:web -> _run_agent_web
    - agent:mcp -> _call_agent_runner
    """
    try:
        _, mode = req.model.split(":", 1)
    except ValueError:
        raise HTTPException(status_code=400, detail="Invalid agent model name")

    mode = mode.strip().lower() or "web"

    if mode == "web":
        return await _run_agent_web(req)

    if mode == "mcp":
        return await _call_agent_runner(req)

    raise HTTPException(status_code=400, detail=f"Unknown agent mode '{mode}'")


# -----------------------------------------------------------------------------
# Routes
# -----------------------------------------------------------------------------
@app.get("/")
async def root():
    return {
        "name": "ai-gateway",
        "ok": True,
        "ollama_base": OLLAMA_BASE,
        "providers_loaded": sorted(list(PROVIDERS.keys())),
        "models_cache_ttl_s": MODELS_CACHE_TTL_S,
        "agent_runner_url": AGENT_RUNNER_URL,
        "circuit_breaker": {
            "fail_threshold": CB_FAIL_THRESHOLD,
            "open_seconds": CB_OPEN_SECS,
        },
    }


@app.get("/health")
async def health():
    ok = await ollama_running()
    return {
        "status": "ok",
        "ollama_reachable": ok,
        "ollama_base": OLLAMA_BASE,
        "time": int(time.time()),
    }


@app.post("/admin/reload")
async def admin_reload():
    providers = _reload_providers_inplace()
    return {"ok": True, "providers_loaded": sorted(list(providers.keys()))}


@app.get("/v1/models")
async def v1_models():
    data: List[Dict[str, Any]] = []

    # Ollama models (no prefix)
    try:
        ollama_names = await list_ollama_models()
        data.extend(
            [
                {"id": n, "object": "model", "created": 0, "owned_by": "ollama"}
                for n in ollama_names
            ]
        )
    except Exception:
        pass

    # Remote providers (prefixed: provider:model_id)
    for name, p in PROVIDERS.items():
        if name == "ollama":
            continue
        if p.type != "openai_compat":
            continue

        models = await _remote_list_models_openai_compat(p)
        for mid in models:
            data.append(
                {
                    "id": f"{name}:{mid}",
                    "object": "model",
                    "created": 0,
                    "owned_by": name,
                }
            )

    # Agent pseudo-models
    data.append({"id": "agent:web", "object": "model", "created": 0, "owned_by": "agent"})
    data.append({"id": "agent:mcp", "object": "model", "created": 0, "owned_by": "agent"})

    return {"object": "list", "data": data}


@app.post("/v1/chat/completions")
async def chat_completions(req: ChatRequest, request: Request):
    created = int(time.time())

    # -------------------------------------------------------------------------
    # Agent models (agent:web, agent:mcp)
    # -------------------------------------------------------------------------
    if req.model.startswith("agent:"):
        # Non-streaming: just delegate and return JSON
        if not req.stream:
            result = await _handle_agent(req)
            return JSONResponse(result)

        # Streaming: run agent non-stream under the hood, then wrap its
        # final answer in a tiny SSE stream so LibreChat is happy.
        tmp_req = ChatRequest(
            model=req.model,
            messages=req.messages,
            stream=False,
            temperature=req.temperature,
            top_p=req.top_p,
            max_tokens=req.max_tokens,
            stop=req.stop,
            presence_penalty=req.presence_penalty,
            frequency_penalty=req.frequency_penalty,
        )
        result = await _handle_agent(tmp_req)

        # Extract content from the OpenAI-style response
        model = result.get("model", req.model)
        created2 = result.get("created", created)
        choices = result.get("choices") or []
        content = ""
        if choices and isinstance(choices[0], dict):
            msg = choices[0].get("message") or {}
            content = (msg or {}).get("content") or ""

        completion_id = result.get("id") or _openai_completion_id()

        async def event_gen():
            # initial role chunk
            yield _sse(
                {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created2,
                    "model": model,
                    "choices": [
                        {"index": 0, "delta": {"role": "assistant"}, "finish_reason": None}
                    ],
                }
            )

            # full content chunk
            if content:
                yield _sse(
                    {
                        "id": completion_id,
                        "object": "chat.completion.chunk",
                        "created": created2,
                        "model": model,
                        "choices": [
                            {
                                "index": 0,
                                "delta": {"content": content},
                                "finish_reason": "stop",
                            }
                        ],
                    }
                )

            # [DONE]
            yield "data: [DONE]\n\n"

        return StreamingResponse(
            event_gen(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
            },
        )

    # -------------------------------------------------------------------------
    # Normal routing: remote providers vs ollama
    # -------------------------------------------------------------------------
    provider_name, model_name = _split_model(req.model)

    # Remote OpenAI-compatible providers
    if provider_name != "ollama":
        p = PROVIDERS.get(provider_name)
        if not p:
            raise HTTPException(status_code=400, detail=f"Unknown provider '{provider_name}'")

        if p.type != "openai_compat":
            raise HTTPException(status_code=400, detail=f"Provider '{provider_name}' is not openai_compat")

        if p.api_key_env and not p.api_key():
            raise HTTPException(
                status_code=400,
                detail={
                    "error": f"Missing API key for provider '{provider_name}'.",
                    "suggestion": f"Set {p.api_key_env} in providers.env and restart the gateway.",
                },
            )

        if not req.stream:
            return JSONResponse(await _remote_chat_openai_compat_nonstream(p, model_name, req))

        async def passthrough():
            async for b in _remote_chat_openai_compat_stream_passthrough(p, model_name, req):
                if await request.is_disconnected():
                    return
                yield b

        return StreamingResponse(
            passthrough(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
            },
        )

    # -------------------------------------------------------------------------
    # Ollama path (convert to OpenAI-ish)
    # -------------------------------------------------------------------------
    messages = [{"role": m.role, "content": m.content} for m in req.messages]
    model = await choose_ollama_model(model_name)

    if not req.stream:
        try:
            out = await ollama_chat_nonstream(model, messages)
        except httpx.TimeoutException:
            raise HTTPException(status_code=504, detail="Timed out waiting for Ollama.")
        except httpx.HTTPError as e:
            raise HTTPException(status_code=502, detail=f"Ollama error: {e}")

        msg = out.get("message") if isinstance(out, dict) else None
        content = msg.get("content", "") if isinstance(msg, dict) else ""
        if not isinstance(content, str):
            content = ""

        return JSONResponse(_openai_nonstream_response(model=model, content=content, created=created))

    async def event_gen():
        completion_id = _openai_completion_id()

        # initial role chunk
        yield _sse(
            {
                "id": completion_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{"index": 0, "delta": {"role": "assistant"}, "finish_reason": None}],
            }
        )

        try:
            async for obj in ollama_chat_stream_ndjson(model, messages):
                if await request.is_disconnected():
                    return

                if not isinstance(obj, dict):
                    continue

                done = bool(obj.get("done", False))
                msg = obj.get("message")
                delta_text = ""
                if isinstance(msg, dict) and isinstance(msg.get("content"), str):
                    delta_text = msg["content"]

                if delta_text:
                    yield _sse(
                        {
                            "id": completion_id,
                            "object": "chat.completion.chunk",
                            "created": created,
                            "model": model,
                            "choices": [
                                {
                                    "index": 0,
                                    "delta": {"content": delta_text},
                                    "finish_reason": None,
                                }
                            ],
                        }
                    )

                if done:
                    yield _sse(
                        {
                            "id": completion_id,
                            "object": "chat.completion.chunk",
                            "created": created,
                            "model": model,
                            "choices": [
                                {
                                    "index": 0,
                                    "delta": {},
                                    "finish_reason": "stop",
                                }
                            ],
                        }
                    )
                    yield "data: [DONE]\n\n"
                    return

        except httpx.TimeoutException:
            yield _sse(
                {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": model,
                    "choices": [
                        {
                            "index": 0,
                            "delta": {},
                            "finish_reason": "timeout",
                        }
                    ],
                }
            )
            yield "data: [DONE]\n\n"
            return
        except Exception as e:
            yield _sse(
                {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": model,
                    "choices": [
                        {
                            "index": 0,
                            "delta": {"content": f"\n[Gateway error] {e}"},
                            "finish_reason": "error",
                        }
                    ],
                }
            )
            yield "data: [DONE]\n\n"
            return

    return StreamingResponse(
        event_gen(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )