# ~/ai/router/router.py
from __future__ import annotations

import json
import os
import time
import uuid
from dataclasses import dataclass, field
from typing import Any, Dict, Iterable, List, Optional, Tuple

import httpx
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel

# -----------------------------------------------------------------------------
# Config
# -----------------------------------------------------------------------------
OLLAMA_BASE = os.getenv("OLLAMA_BASE", "http://127.0.0.1:11434").rstrip("/")
DEFAULT_FALLBACK_MODEL = os.getenv("DEFAULT_FALLBACK_MODEL", "mistral:latest")

PROVIDERS_YAML = os.getenv("PROVIDERS_YAML", "/Users/bee/ai/providers.yaml")

TAGS_TIMEOUT_S = float(os.getenv("OLLAMA_TAGS_TIMEOUT", "3.0"))
CHAT_TIMEOUT_S = float(os.getenv("OLLAMA_CHAT_TIMEOUT", "180.0"))
REMOTE_TIMEOUT_S = float(os.getenv("REMOTE_TIMEOUT", "180.0"))
REMOTE_MODELS_TIMEOUT_S = float(os.getenv("REMOTE_MODELS_TIMEOUT", "10.0"))

app = FastAPI(title="AI Gateway", version="0.2.0")

# -----------------------------------------------------------------------------
# OpenAI-ish request models
# -----------------------------------------------------------------------------
class Message(BaseModel):
    role: str
    content: str


class ChatRequest(BaseModel):
    model: str
    messages: List[Message]
    stream: bool = False

    # optional OpenAI-ish fields (passed through to OpenAI-compat providers)
    temperature: Optional[float] = None
    top_p: Optional[float] = None
    max_tokens: Optional[int] = None

    # extra common fields users may send (ignored unless provider supports them)
    stop: Optional[Any] = None
    presence_penalty: Optional[float] = None
    frequency_penalty: Optional[float] = None


# -----------------------------------------------------------------------------
# Providers
# -----------------------------------------------------------------------------
@dataclass
class Provider:
    name: str
    type: str
    base_url: str
    api_key_env: Optional[str] = None
    default_headers: Dict[str, str] = field(default_factory=dict)
    models: List[str] = field(default_factory=list)  # optional fallback list

    def api_key(self) -> Optional[str]:
        if not self.api_key_env:
            return None
        return os.getenv(self.api_key_env)


def _load_providers() -> Dict[str, Provider]:
    providers: Dict[str, Provider] = {}

    # Always include local ollama (even if not in YAML)
    providers["ollama"] = Provider(
        name="ollama",
        type="ollama",
        base_url=OLLAMA_BASE,
        api_key_env=None,
    )

    if not PROVIDERS_YAML or not os.path.exists(PROVIDERS_YAML):
        return providers

    try:
        import yaml  # type: ignore
    except Exception as e:
        raise RuntimeError(
            f"PROVIDERS_YAML is set to {PROVIDERS_YAML}, but PyYAML is not installed. "
            f"Install it in your venv: pip install pyyaml. Underlying error: {e}"
        )

    with open(PROVIDERS_YAML, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f) or {}

    root = data.get("providers", {}) if isinstance(data, dict) else {}
    if not isinstance(root, dict):
        return providers

    for name, cfg in root.items():
        if not isinstance(name, str) or not isinstance(cfg, dict):
            continue

        ptype = str(cfg.get("type", "")).strip()
        base_url = str(cfg.get("base_url", "")).strip().rstrip("/")
        if not ptype or not base_url:
            continue

        api_key_env = cfg.get("api_key_env")
        if api_key_env is not None:
            api_key_env = str(api_key_env).strip()

        default_headers = cfg.get("default_headers") or {}
        if not isinstance(default_headers, dict):
            default_headers = {}

        models = cfg.get("models") or []
        if not isinstance(models, list):
            models = []
        models = [str(m) for m in models if isinstance(m, (str, int, float))]

        providers[name] = Provider(
            name=name,
            type=ptype,
            base_url=base_url,
            api_key_env=api_key_env,
            default_headers={str(k): str(v) for k, v in default_headers.items()},
            models=models,
        )

    return providers


PROVIDERS: Dict[str, Provider] = _load_providers()

# -----------------------------------------------------------------------------
# Ollama helpers
# -----------------------------------------------------------------------------
async def _get_tags() -> Dict[str, Any]:
    url = f"{OLLAMA_BASE}/api/tags"
    async with httpx.AsyncClient(timeout=TAGS_TIMEOUT_S) as client:
        r = await client.get(url)
        r.raise_for_status()
        return r.json()


async def ollama_running() -> bool:
    try:
        await _get_tags()
        return True
    except Exception:
        return False


async def list_ollama_models() -> List[str]:
    payload = await _get_tags()
    models = payload.get("models", [])
    if not isinstance(models, list):
        return []
    names: List[str] = []
    for m in models:
        if isinstance(m, dict) and isinstance(m.get("name"), str):
            names.append(m["name"])
    return names


def _normalize_ollama_model(requested: str, available: List[str]) -> Tuple[bool, str]:
    if requested in available:
        return True, requested
    if ":" not in requested:
        cand = f"{requested}:latest"
        if cand in available:
            return True, cand
    return False, requested


async def choose_ollama_model(requested: str) -> str:
    try:
        available = await list_ollama_models()
    except Exception as e:
        raise HTTPException(status_code=502, detail=f"Failed to reach Ollama at {OLLAMA_BASE}: {e}")

    ok, actual = _normalize_ollama_model(requested, available)
    if ok:
        return actual

    if DEFAULT_FALLBACK_MODEL:
        ok2, actual2 = _normalize_ollama_model(DEFAULT_FALLBACK_MODEL, available)
        if ok2:
            return actual2

    raise HTTPException(
        status_code=400,
        detail={
            "error": f"Model '{requested}' not found, and fallback '{DEFAULT_FALLBACK_MODEL}' is not available.",
            "suggestion": "Run `ollama list` to view installed models.",
        },
    )


async def ollama_chat_nonstream(model: str, messages: List[Dict[str, str]]) -> Dict[str, Any]:
    url = f"{OLLAMA_BASE}/api/chat"
    payload = {"model": model, "messages": messages, "stream": False}
    async with httpx.AsyncClient(timeout=CHAT_TIMEOUT_S) as client:
        r = await client.post(url, json=payload)
        r.raise_for_status()
        return r.json()


async def ollama_chat_stream_ndjson(model: str, messages: List[Dict[str, str]]):
    url = f"{OLLAMA_BASE}/api/chat"
    payload = {"model": model, "messages": messages, "stream": True}
    async with httpx.AsyncClient(timeout=None) as client:
        async with client.stream("POST", url, json=payload, timeout=CHAT_TIMEOUT_S) as resp:
            resp.raise_for_status()
            async for line in resp.aiter_lines():
                if not line:
                    continue
                try:
                    yield json.loads(line)
                except json.JSONDecodeError:
                    continue


# -----------------------------------------------------------------------------
# OpenAI-ish helpers (for Ollama conversion)
# -----------------------------------------------------------------------------
def _openai_completion_id() -> str:
    return f"chatcmpl-{uuid.uuid4().hex}"


def _openai_nonstream_response(model: str, content: str, created: int) -> Dict[str, Any]:
    return {
        "id": _openai_completion_id(),
        "object": "chat.completion",
        "created": created,
        "model": model,
        "choices": [
            {
                "index": 0,
                "message": {"role": "assistant", "content": content},
                "finish_reason": "stop",
            }
        ],
    }


def _sse(data_obj: Any) -> str:
    return f"data: {json.dumps(data_obj, ensure_ascii=False)}\n\n"


# -----------------------------------------------------------------------------
# OpenAI-compat remote provider helpers
# -----------------------------------------------------------------------------
def _split_model(model: str) -> Tuple[str, str]:
    """
    Returns (provider_name, model_name).

    Rules:
      - "openrouter:meta-llama/llama-3.1-8b-instruct" -> ("openrouter", "meta-llama/llama-3.1-8b-instruct")
      - "llama3:8b" -> ("ollama", "llama3:8b")   (default)
    """
    if ":" in model:
        pfx, rest = model.split(":", 1)
        if pfx in PROVIDERS and pfx != "ollama":
            return pfx, rest
    return "ollama", model


def _auth_headers(p: Provider) -> Dict[str, str]:
    headers = {"Content-Type": "application/json"}
    headers.update(p.default_headers or {})
    key = p.api_key()
    if key:
        headers["Authorization"] = f"Bearer {key}"
    return headers


async def _remote_list_models_openai_compat(p: Provider) -> List[str]:
    # Best-effort: use /models if it exists, else fall back to p.models
    if p.models:
        fallback = list(p.models)
    else:
        fallback = []

    key = p.api_key()
    if p.api_key_env and not key:
        return fallback

    url = f"{p.base_url}/models"
    try:
        async with httpx.AsyncClient(timeout=REMOTE_MODELS_TIMEOUT_S) as client:
            r = await client.get(url, headers=_auth_headers(p))
            if r.status_code >= 400:
                return fallback
            data = r.json()
            out: List[str] = []
            items = data.get("data", [])
            if isinstance(items, list):
                for it in items:
                    if isinstance(it, dict) and isinstance(it.get("id"), str):
                        out.append(it["id"])
            return out or fallback
    except Exception:
        return fallback


async def _remote_chat_openai_compat_nonstream(p: Provider, model: str, req: ChatRequest) -> Dict[str, Any]:
    url = f"{p.base_url}/chat/completions"
    payload: Dict[str, Any] = {
        "model": model,
        "messages": [{"role": m.role, "content": m.content} for m in req.messages],
        "stream": False,
    }
    # pass through common optional fields if provided
    for k in ("temperature", "top_p", "max_tokens", "stop", "presence_penalty", "frequency_penalty"):
        v = getattr(req, k, None)
        if v is not None:
            payload[k] = v

    async with httpx.AsyncClient(timeout=REMOTE_TIMEOUT_S) as client:
        r = await client.post(url, headers=_auth_headers(p), json=payload)
        if r.status_code >= 400:
            # surface provider response if possible
            try:
                raise HTTPException(status_code=r.status_code, detail=r.json())
            except Exception:
                raise HTTPException(status_code=r.status_code, detail=r.text)
        return r.json()


async def _remote_chat_openai_compat_stream_passthrough(p: Provider, model: str, req: ChatRequest):
    url = f"{p.base_url}/chat/completions"
    payload: Dict[str, Any] = {
        "model": model,
        "messages": [{"role": m.role, "content": m.content} for m in req.messages],
        "stream": True,
    }
    for k in ("temperature", "top_p", "max_tokens", "stop", "presence_penalty", "frequency_penalty"):
        v = getattr(req, k, None)
        if v is not None:
            payload[k] = v

    async with httpx.AsyncClient(timeout=None) as client:
        async with client.stream("POST", url, headers=_auth_headers(p), json=payload, timeout=REMOTE_TIMEOUT_S) as resp:
            if resp.status_code >= 400:
                try:
                    detail = await resp.aread()
                    raise HTTPException(status_code=resp.status_code, detail=detail.decode("utf-8", "ignore"))
                except HTTPException:
                    raise
                except Exception:
                    raise HTTPException(status_code=resp.status_code, detail="Remote provider error")

            async for chunk in resp.aiter_bytes():
                if chunk:
                    yield chunk


# -----------------------------------------------------------------------------
# Routes
# -----------------------------------------------------------------------------
@app.get("/")
async def root():
    return {
        "name": "ai-gateway",
        "ok": True,
        "ollama_base": OLLAMA_BASE,
        "providers_loaded": sorted(list(PROVIDERS.keys())),
    }


@app.get("/health")
async def health():
    ok = await ollama_running()
    return {
        "status": "ok",
        "ollama_reachable": ok,
        "ollama_base": OLLAMA_BASE,
        "time": int(time.time()),
    }


@app.get("/v1/models")
async def v1_models():
    data: List[Dict[str, Any]] = []

    # Ollama models (no prefix)
    try:
        ollama_names = await list_ollama_models()
        data.extend([{"id": n, "object": "model", "created": 0, "owned_by": "ollama"} for n in ollama_names])
    except Exception:
        pass

    # Remote providers (prefixed: provider:model_id)
    for name, p in PROVIDERS.items():
        if name == "ollama":
            continue
        if p.type != "openai_compat":
            continue

        models = await _remote_list_models_openai_compat(p)
        for mid in models:
            data.append({"id": f"{name}:{mid}", "object": "model", "created": 0, "owned_by": name})

    return {"object": "list", "data": data}


@app.post("/v1/chat/completions")
async def chat_completions(req: ChatRequest, request: Request):
    created = int(time.time())
    provider_name, model_name = _split_model(req.model)

    # -------------------------------------------------------------------------
    # Remote OpenAI-compatible providers
    # -------------------------------------------------------------------------
    if provider_name != "ollama":
        p = PROVIDERS.get(provider_name)
        if not p:
            raise HTTPException(status_code=400, detail=f"Unknown provider '{provider_name}'")

        if p.type != "openai_compat":
            raise HTTPException(status_code=400, detail=f"Provider '{provider_name}' is not openai_compat")

        if p.api_key_env and not p.api_key():
            raise HTTPException(
                status_code=400,
                detail={
                    "error": f"Missing API key for provider '{provider_name}'.",
                    "suggestion": f"Set {p.api_key_env} in providers.env and restart the LaunchAgent.",
                },
            )

        if not req.stream:
            return JSONResponse(await _remote_chat_openai_compat_nonstream(p, model_name, req))

        async def passthrough():
            async for b in _remote_chat_openai_compat_stream_passthrough(p, model_name, req):
                if await request.is_disconnected():
                    return
                yield b

        return StreamingResponse(
            passthrough(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no",
            },
        )

    # -------------------------------------------------------------------------
    # Ollama path (convert to OpenAI-ish)
    # -------------------------------------------------------------------------
    messages = [{"role": m.role, "content": m.content} for m in req.messages]
    model = await choose_ollama_model(model_name)

    if not req.stream:
        try:
            out = await ollama_chat_nonstream(model, messages)
        except httpx.TimeoutException:
            raise HTTPException(status_code=504, detail="Timed out waiting for Ollama.")
        except httpx.HTTPError as e:
            raise HTTPException(status_code=502, detail=f"Ollama error: {e}")

        msg = out.get("message") if isinstance(out, dict) else None
        content = msg.get("content", "") if isinstance(msg, dict) else ""
        if not isinstance(content, str):
            content = ""

        return JSONResponse(_openai_nonstream_response(model=model, content=content, created=created))

    async def event_gen():
        completion_id = _openai_completion_id()

        # initial role chunk
        yield _sse(
            {
                "id": completion_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": model,
                "choices": [{"index": 0, "delta": {"role": "assistant"}, "finish_reason": None}],
            }
        )

        try:
            async for obj in ollama_chat_stream_ndjson(model, messages):
                if await request.is_disconnected():
                    return

                if not isinstance(obj, dict):
                    continue

                done = bool(obj.get("done", False))
                msg = obj.get("message")
                delta_text = ""
                if isinstance(msg, dict) and isinstance(msg.get("content"), str):
                    delta_text = msg["content"]

                if delta_text:
                    yield _sse(
                        {
                            "id": completion_id,
                            "object": "chat.completion.chunk",
                            "created": created,
                            "model": model,
                            "choices": [{"index": 0, "delta": {"content": delta_text}, "finish_reason": None}],
                        }
                    )

                if done:
                    yield _sse(
                        {
                            "id": completion_id,
                            "object": "chat.completion.chunk",
                            "created": created,
                            "model": model,
                            "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}],
                        }
                    )
                    yield "data: [DONE]\n\n"
                    return

        except httpx.TimeoutException:
            yield _sse(
                {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": model,
                    "choices": [{"index": 0, "delta": {}, "finish_reason": "timeout"}],
                }
            )
            yield "data: [DONE]\n\n"
            return
        except Exception as e:
            yield _sse(
                {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created,
                    "model": model,
                    "choices": [{"index": 0, "delta": {"content": f"\n[Gateway error] {e}"}, "finish_reason": "error"}],
                }
            )
            yield "data: [DONE]\n\n"
            return

    return StreamingResponse(
        event_gen(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )